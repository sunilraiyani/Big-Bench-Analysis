## Cluster Environment

**SSH**

make sure passwordless SSH is working between all nodes for you current user.

**Java**

Java 1.7 is required. 64 bit is recommended

**Hadoop**

* Hive

## Installation

On the AWS installation, the github repository is cloned into a folder "nfs":

```
cd
mkdir nfs
cd nfs
git clone https://<username>@github.com/intel-hadoop/Big-Bench.git
```

Edit .bashrc:

```
cd
vi .bashrc
```

and add these lines to source the environment setup file:

```
if [ -f ~/nfs/Big-Bench/setEnvVars ]; then
        . ~/nfs/Big-Bench/setEnvVars
fi
```

Either logout/login or source the environment script manually to set the required variables:

`source ~/nfs/Big-Bench/setEnvVars`

### Configuration

Check, if the hadoop related variables are correctly set in the environment file:

`vi ~/nfs/Big-Bench/setEnvVars`

**Important:**

If you changed something you must either logout/login or source the setEnvVars script manually to make you changes visible to the environment. e.g:
`source ~/nfs/Big-Bench/setEnvVars`


Major settings, Specify your cluster environment:

```
BIG_BENCH_HIVE_LIBS
BIG_BENCH_HADOOP_LIBS
BIG_BENCH_HADOOP_LIBS_NATIVE
BIG_BENCH_HADOOP_CONF
BIG_BENCH_HDFS_MOUNT_POINT
BIG_BENCH_HDFS_NAMENODE
```
Minor settings:
```
BIG_BENCH_USER
BIG_BENCH_DATAGEN_DFS_REPLICATION
```


Add the nodes on which PDGF should generate data into the nodes.txt file:

`vi $BIG_BENCH_BASH_SCRIPT_DIR/nodes.txt`

In this file, list all hosts, one per line:

```
bb-aws1
bb-aws2
bb-aws3
bb-aws4
```

**Important:** As a temporary measure (until PDGF can be run as a hadoop job), the directory structure must be replicated onto all nodes. So either repeat the "git clone" on every node (make sure that the directory structure is the same) or export the ~/nfs folder as a nfs share and mount it on all nodes in the ec2-user's ~/nfs directory (this is what we did on the AWS nodes). As a shared medium eases the following steps significantly, that approach is strongly recommended.

## Data Generation
### First run
Before the first PDGF run, the license must be accepted once. Therefore, PDGF must be started:

`java -jar $BIG_BENCH_DATA_GENERATOR_DIR/pdgf.jar `

Pressing ENTER shows the license. The license must be accepted by entering 'y'. After that, the PDGF shell can be exited by pressing 'q':

```
By using this software you must first agree to our terms of use. press [ENTER] to show
...
Do you agree to this terms of use [Y/N]?
y

PDGF:> q
```

**Important:** The license must be accepted in every PDGF location. So if no shared medium is used, the license must be accepted in every PDGF copy on all nodes.

### Distributed generation
To generate data on the cluster nodes, run this command:

`$BIG_BENCH_BASH_SCRIPT_DIR/bigBenchClusterDataGen.sh`
or relative to your cwd:
`./scripts/bigBenchClusterDataGen.sh`

The data are being generated directly into HDFS (into the benchmarks/bigbench/data/ directory, absolute HDFS path is /user/ec2-user/benchmarks/bigbench/data/).

Default HDFS replication count is 1 (data is onyl stored on the generating node). You can change this in
`setEnvVars BIG_BENCH_DATAGEN_DFS_REPLICATION=<Replication count>' as described in: [Configuration](#Configuration)

Hive population is automatically done after the data generation is finished.

## Hive Population
Hive must create its own metadata to be able to access the generated data. 
Hive population is automatically done after the data generation with `bigBenchClusterDataGen.sh`.
In case you want/must renew the hive tables: the following command drops old tables and recreates them with the output data from PDGF:

`hive -f $BIG_BENCH_HIVE_SCRIPT_DIR/hiveCreateLoad.hql`

## Run Queries

Run one specific query with this command:
`/$BIG_BENCH_QUERIES_DIR/q<number>/run.sh`
e.g:
`/$BIG_BENCH_QUERIES_DIR/q01/run.sh`

or relative to your working directory:
`./queries/q<number>/run.sh`

Run all queries with this command:
`find $BIG_BENCH_QUERIES_DIR -name 'run.sh' -exec {} \;`